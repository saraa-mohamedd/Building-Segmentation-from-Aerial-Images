{"cells":[{"cell_type":"markdown","metadata":{},"source":["## Introduction\n","\n","### In this notebook we use [UNet](https://arxiv.org/abs/1505.04597) segmentation model for performing building segmentation"]},{"cell_type":"markdown","metadata":{},"source":["### Libraries üìö‚¨á"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os, cv2\n","import numpy as np\n","import pandas as pd\n","import random, tqdm\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","import albumentations as album\n","\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip install -q -U segmentation-models-pytorch albumentations > /dev/null\n","import segmentation_models_pytorch as smp"]},{"cell_type":"markdown","metadata":{},"source":["### Defining train / val / test directories üìÅ"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["DATA_DIR = '/home/sara/Building-Segmentation-DL/data/'\n","\n","# Define the paths\n","image_dir = '/home/sara/Building-Segmentation-DL/data/3band/'\n","ground_truth_dir = '/home/sara/Building-Segmentation-DL/data/buildingMaskLabels/'\n","text_dir = '/home/sara/Building-Segmentation-DL/data/dataSplit/'\n","\n","\n","# Define the names of the text files\n","text_files = ['test.txt', 'train.txt', 'val.txt']\n","\n","# Define the names of the directories\n","dir_names = [ 'train', 'val', 'test']\n","\n","# Read the text files\n","def split_images(image_dir, text_dir, text_files, dir_names):\n","    for text_file, dir_name in zip(text_files, dir_names):\n","        with open(os.path.join(text_dir, text_file), 'r') as file:\n","            image_names = file.read().splitlines()\n","            print(image_names)\n","    # Create the directory if it doesn't exist\n","        os.makedirs(os.path.join(image_dir, dir_name), exist_ok=True)\n","\n","    # Iterate over the images in the image directory\n","        for image in os.listdir(image_dir):\n","            if image in image_names:\n","            # Move the image to the corresponding directory\n","                shutil.move(os.path.join(image_dir, image), os.path.join(image_dir, dir_name, image))\n","\n","# split input images to test, train, and validation\n","split_images(image_dir, text_dir, text_files, dir_names)\n","\n","# split ground truth images to test, train, and validation\n","\n","split_images(ground_truth_dir, text_dir, text_files, dir_names)\n","\n","x_train_dir = os.path.join(image_dir, dir_names[0])\n","y_train_dir = os.path.join(ground_truth_dir, dir_names[0])\n","\n","x_valid_dir = os.path.join(image_dir, dir_names[1])\n","y_valid_dir = os.path.join(ground_truth_dir, dir_names[1])\n","\n","x_test_dir = os.path.join(image_dir, dir_names[2])\n","y_test_dir = os.path.join(ground_truth_dir, dir_names[2])\n","\n","# DATA_DIR = '../input/massachusetts-buildings-dataset/tiff/'\n","\n","# x_train_dir = os.path.join(DATA_DIR, 'train')\n","# y_train_dir = os.path.join(DATA_DIR, 'train_labels')\n","\n","# x_valid_dir = os.path.join(DATA_DIR, 'val')\n","# y_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n","\n","# x_test_dir = os.path.join(DATA_DIR, 'test')\n","# y_test_dir = os.path.join(DATA_DIR, 'test_labels')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class_dict = pd.read_csv(\"./input/massachusetts-buildings-dataset/label_class_dict.csv\")\n","# Get class names\n","class_names = class_dict['name'].tolist()\n","# Get class RGB values\n","class_rgb_values = class_dict[['r','g','b']].values.tolist()\n","\n","print('All dataset classes and their corresponding RGB values in labels:')\n","print('Class Names: ', class_names)\n","print('Class RGB values: ', class_rgb_values)"]},{"cell_type":"markdown","metadata":{},"source":["#### Shortlist specific classes to segment"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Useful to shortlist specific classes in datasets with large number of classes\n","select_classes = ['background', 'building']\n","\n","# Get RGB values of required classes\n","select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n","select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n","\n","print('Selected classes and their corresponding RGB values in labels:')\n","print('Class Names: ', class_names)\n","print('Class RGB values: ', class_rgb_values)"]},{"cell_type":"markdown","metadata":{},"source":["### Helper functions for viz. & one-hot encoding/decoding"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# helper function for data visualization\n","def visualize(**images):\n","    \"\"\"\n","    Plot images in one row\n","    \"\"\"\n","    n_images = len(images)\n","    plt.figure(figsize=(20,8))\n","    for idx, (name, image) in enumerate(images.items()):\n","        plt.subplot(1, n_images, idx + 1)\n","        plt.xticks([]); \n","        plt.yticks([])\n","        # get title from the parameter names\n","        plt.title(name.replace('_',' ').title(), fontsize=20)\n","        plt.imshow(image)\n","    plt.show()\n","\n","# Perform one hot encoding on label\n","def one_hot_encode(label, label_values):\n","    \"\"\"\n","    Convert a segmentation image label array to one-hot format\n","    by replacing each pixel value with a vector of length num_classes\n","    # Arguments\n","        label: The 2D array segmentation image label\n","        label_values\n","        \n","    # Returns\n","        A 2D array with the same width and hieght as the input, but\n","        with a depth size of num_classes\n","    \"\"\"\n","    semantic_map = []\n","    for colour in label_values:\n","        equality = np.equal(label, colour)\n","        class_map = np.all(equality, axis = -1)\n","        semantic_map.append(class_map)\n","    semantic_map = np.stack(semantic_map, axis=-1)\n","\n","    return semantic_map\n","    \n","# Perform reverse one-hot-encoding on labels / preds\n","def reverse_one_hot(image):\n","    \"\"\"\n","    Transform a 2D array in one-hot format (depth is num_classes),\n","    to a 2D array with only 1 channel, where each pixel value is\n","    the classified class key.\n","    # Arguments\n","        image: The one-hot format image \n","        \n","    # Returns\n","        A 2D array with the same width and hieght as the input, but\n","        with a depth size of 1, where each pixel value is the classified \n","        class key.\n","    \"\"\"\n","    x = np.argmax(image, axis = -1)\n","    return x\n","\n","# Perform colour coding on the reverse-one-hot outputs\n","def colour_code_segmentation(image, label_values):\n","    \"\"\"\n","    Given a 1-channel array of class keys, colour code the segmentation results.\n","    # Arguments\n","        image: single channel array where each value represents the class key.\n","        label_values\n","\n","    # Returns\n","        Colour coded image for segmentation visualization\n","    \"\"\"\n","    colour_codes = np.array(label_values)\n","    x = colour_codes[image.astype(int)]\n","\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class BuildingsDataset(torch.utils.data.Dataset):\n","\n","    \"\"\"Massachusetts Buildings Dataset. Read images, apply augmentation and preprocessing transformations.\n","    \n","    Args:\n","        images_dir (str): path to images folder\n","        masks_dir (str): path to segmentation masks folder\n","        class_rgb_values (list): RGB values of select classes to extract from segmentation mask\n","        augmentation (albumentations.Compose): data transfromation pipeline \n","            (e.g. flip, scale, etc.)\n","        preprocessing (albumentations.Compose): data preprocessing \n","            (e.g. noralization, shape manipulation, etc.)\n","    \n","    \"\"\"\n","    \n","    def __init__(\n","            self, \n","            images_dir, \n","            masks_dir, \n","            class_rgb_values=None, \n","            augmentation=None, \n","            preprocessing=None,\n","            type=\"None\",\n","    ):\n","        \n","        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n","        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n","\n","        self.class_rgb_values = class_rgb_values\n","        self.augmentation = augmentation\n","        self.preprocessing = preprocessing\n","        self.type = type\n","    \n","    def __getitem__(self, i):\n","        \n","        # read images and masks\n","        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n","        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n","        \n","         # Resize the image to 1500 pixels\n","        image = cv2.resize(image, (1500, 1500))\n","        mask = cv2.resize(mask, (1500, 1500))\n","\n","        # one-hot-encode the mask\n","        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n","        \n","        # apply augmentations\n","        if self.type == \"training\":\n","            image = cv2.resize(image, (256, 256))\n","            mask = cv2.resize(mask, (256, 256))\n","        if self.augmentation:\n","            sample = self.augmentation(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","        # apply preprocessing\n","        if self.preprocessing:\n","            sample = self.preprocessing(image=image, mask=mask)\n","            image, mask = sample['image'], sample['mask']\n","            \n","        return image, mask\n","        \n","    def __len__(self):\n","        # return length of \n","        return len(self.image_paths)"]},{"cell_type":"markdown","metadata":{},"source":["#### Visualize Sample Image and Mask üìà"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = BuildingsDataset(x_train_dir, y_train_dir, class_rgb_values=select_class_rgb_values, type=\"None\")\n","random_idx = random.randint(0, len(dataset)-1)\n","image, mask = dataset[random_idx]\n","\n","visualize(\n","    original_image = image,\n","    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n","    one_hot_encoded_mask = reverse_one_hot(mask)\n",")\n","\n","print(image.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Defining Augmentations üôÉ"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def get_training_augmentation():\n","    train_transform = [    \n","        # album.resize(256, 256),\n","        # album.RandomCrop(height=256, width=256, always_apply=True),\n","        album.OneOf(\n","            [\n","                album.HorizontalFlip(p=1),\n","                album.VerticalFlip(p=1),\n","                album.RandomRotate90(p=1),\n","            ],\n","            p=0.75,\n","        ),\n","    ]\n","    return album.Compose(train_transform)\n","\n","\n","def get_validation_augmentation():   \n","    # Add sufficient padding to ensure image is divisible by 32\n","    test_transform = [\n","        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n","    ]\n","    return album.Compose(test_transform)\n","\n","\n","def to_tensor(x, **kwargs):\n","    return x.transpose(2, 0, 1).astype('float32')\n","\n","\n","def get_preprocessing(preprocessing_fn=None):\n","    \"\"\"Construct preprocessing transform    \n","    Args:\n","        preprocessing_fn (callable): data normalization function \n","            (can be specific for each pretrained neural network)\n","    Return:\n","        transform: albumentations.Compose\n","    \"\"\"   \n","    _transform = []\n","    if preprocessing_fn:\n","        _transform.append(album.Lambda(image=preprocessing_fn))\n","    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n","        \n","    return album.Compose(_transform)"]},{"cell_type":"markdown","metadata":{},"source":["#### Visualize Augmented Images & Masks"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["augmented_dataset = BuildingsDataset(\n","    x_train_dir, y_train_dir, \n","    augmentation=get_training_augmentation(),\n","    class_rgb_values=select_class_rgb_values,\n","    type=\"training\"\n",")\n","\n","random_idx = random.randint(0, len(augmented_dataset)-1)\n","\n","# Different augmentations on a random image/mask pair (256*256 crop)\n","for i in range(3):\n","    image, mask = augmented_dataset[random_idx]\n","    visualize(\n","        original_image = image,\n","        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n","        one_hot_encoded_mask = reverse_one_hot(mask)\n","    )\n","    print(image.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Training UNet"]},{"cell_type":"markdown","metadata":{},"source":["<h3><center>UNet Model Architecture</center></h3>\n","<img src=\"https://miro.medium.com/max/2824/1*f7YOaE4TWubwaFF7Z1fzNw.png\" width=\"750\" height=\"750\"/>\n","<h4><center><a href=\"https://arxiv.org/abs/1505.04597\">Image Courtesy: UNet [Ronneberger et al.]</a></center></h4>"]},{"cell_type":"markdown","metadata":{},"source":["### Model Definition"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DoubleConv, self).__init__()\n","        self.double_conv = nn.Sequential(\n","            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(out_channels),\n","            nn.ReLU(inplace=True),\n","        )\n","\n","    def forward(self, x):\n","        return self.double_conv(x)\n","    \n","    \n","class DownBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super(DownBlock, self).__init__()\n","        self.double_conv = DoubleConv(in_channels, out_channels)\n","        self.down_sample = nn.MaxPool2d(2)\n","\n","    def forward(self, x):\n","        skip_out = self.double_conv(x)\n","        down_out = self.down_sample(skip_out)\n","        return (down_out, skip_out)\n","\n","    \n","class UpBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, up_sample_mode):\n","        super(UpBlock, self).__init__()\n","        if up_sample_mode == 'conv_transpose':\n","            self.up_sample = nn.ConvTranspose2d(in_channels-out_channels, in_channels-out_channels, kernel_size=2, stride=2)        \n","        elif up_sample_mode == 'bilinear':\n","            self.up_sample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n","        else:\n","            raise ValueError(\"Unsupported `up_sample_mode` (can take one of `conv_transpose` or `bilinear`)\")\n","        self.double_conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, down_input, skip_input):\n","        x = self.up_sample(down_input)\n","        x = torch.cat([x, skip_input], dim=1)\n","        return self.double_conv(x)\n","\n","    \n","class UNet(nn.Module):\n","    def __init__(self, out_classes=2, up_sample_mode='conv_transpose'):\n","        super(UNet, self).__init__()\n","        self.up_sample_mode = up_sample_mode\n","        # Downsampling Path\n","        self.down_conv1 = DownBlock(3, 64)\n","        self.down_conv2 = DownBlock(64, 128)\n","        self.down_conv3 = DownBlock(128, 256)\n","        self.down_conv4 = DownBlock(256, 512)\n","        # Bottleneck\n","        self.double_conv = DoubleConv(512, 1024)\n","        # Upsampling Path\n","        self.up_conv4 = UpBlock(512 + 1024, 512, self.up_sample_mode)\n","        self.up_conv3 = UpBlock(256 + 512, 256, self.up_sample_mode)\n","        self.up_conv2 = UpBlock(128 + 256, 128, self.up_sample_mode)\n","        self.up_conv1 = UpBlock(128 + 64, 64, self.up_sample_mode)\n","        # Final Convolution\n","        self.conv_last = nn.Conv2d(64, out_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        x, skip1_out = self.down_conv1(x)\n","        x, skip2_out = self.down_conv2(x)\n","        x, skip3_out = self.down_conv3(x)\n","        x, skip4_out = self.down_conv4(x)\n","        x = self.double_conv(x)\n","        x = self.up_conv4(x, skip4_out)\n","        x = self.up_conv3(x, skip3_out)\n","        x = self.up_conv2(x, skip2_out)\n","        x = self.up_conv1(x, skip1_out)\n","        x = self.conv_last(x)\n","        return x\n","    \n","\n","# Get UNet model\n","model = UNet()"]},{"cell_type":"markdown","metadata":{},"source":["#### Get Train / Val DataLoaders"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Get train and val dataset instances\n","train_dataset = BuildingsDataset(\n","    x_train_dir, y_train_dir, \n","    augmentation=get_training_augmentation(),\n","    preprocessing=get_preprocessing(preprocessing_fn=None),\n","    class_rgb_values=select_class_rgb_values,\n","    type=\"training\",\n",")\n","\n","valid_dataset = BuildingsDataset(\n","    x_valid_dir, y_valid_dir, \n","    augmentation=get_validation_augmentation(), \n","    preprocessing=get_preprocessing(preprocessing_fn=None),\n","    class_rgb_values=select_class_rgb_values,\n","    type=\"validation\",\n",")\n","\n","# Get train and val data loaders\n","train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=12)\n","valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)\n","\n","print(valid_dataset[0][0].shape)\n","print(train_dataset[0][0].shape)\n","print(len(train_dataset))"]},{"cell_type":"markdown","metadata":{},"source":["#### Set Hyperparams"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from segmentation_models_pytorch import utils\n","\n","\n","# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n","TRAINING = True\n","\n","# Set num of epochs\n","EPOCHS = 40\n","\n","# Set device: `cuda` or `cpu`\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# define loss function\n","loss = smp.utils.losses.DiceLoss()\n","\n","# define metrics\n","metrics = [\n","    smp.utils.metrics.IoU(threshold=0.5),\n","    smp.utils.metrics.Fscore(threshold=0.5),\n","    smp.utils.metrics.Accuracy(threshold=0.5),\n","]\n","\n","# define optimizer\n","optimizer = torch.optim.Adam([ \n","    dict(params=model.parameters(), lr=0.00008),\n","])\n","\n","# define learning rate scheduler (not used in this NB)\n","lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n","    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",")\n","\n","# # load best saved model checkpoint from previous commit (if present)\n","# if os.path.exists('../input/unet-for-building-segmentation-pytorch/best_model.pth'):\n","#     model = torch.load('../input/unet-for-building-segmentation-pytorch/best_model.pth', map_location=DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_epoch = smp.utils.train.TrainEpoch(\n","    model, \n","    loss=loss, \n","    metrics=metrics, \n","    optimizer=optimizer,\n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","valid_epoch = smp.utils.train.ValidEpoch(\n","    model, \n","    loss=loss, \n","    metrics=metrics, \n","    device=DEVICE,\n","    verbose=True,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Training UNet"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["%%time\n","\n","if TRAINING:\n","\n","    best_iou_score = 0.0\n","    train_logs_list, valid_logs_list = [], []\n","\n","    for i in range(0, EPOCHS):\n","\n","        # Perform training & validation\n","        print('\\nEpoch: {}'.format(i))\n","        train_logs = train_epoch.run(train_loader)\n","        valid_logs = valid_epoch.run(valid_loader)\n","        train_logs_list.append(train_logs)\n","        valid_logs_list.append(valid_logs)\n","\n","        # Save model if a better val IoU score is obtained\n","        if best_iou_score < valid_logs['iou_score']:\n","            best_iou_score = valid_logs['iou_score']\n","            torch.save(model, './bestnewest__model_{}.pth'.format(i))\n","            print('Model saved!')"]},{"cell_type":"markdown","metadata":{},"source":["### Prediction on Test Data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# load best saved model checkpoint from the current run\n","if os.path.exists('./bestnewest__model_11.pth'):\n","    best_model = torch.load('./bestnewest__model_11.pth', map_location=DEVICE)\n","    print('Loaded UNet model from this run.')\n","\n","# load best saved model checkpoint from previous commit (if present)\n","elif os.path.exists('../input/unet-for-building-segmentation-pytorch/best_model.pth'):\n","    best_model = torch.load('../input/unet-for-building-segmentation-pytorch/best_model.pth', map_location=DEVICE)\n","    print('Loaded UNet model from a previous commit.')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# create test dataloader to be used with UNet model (with preprocessing operation: to_tensor(...))\n","test_dataset = BuildingsDataset(\n","    x_test_dir, \n","    y_test_dir, \n","    augmentation=get_validation_augmentation(), \n","    preprocessing=get_preprocessing(preprocessing_fn=None),\n","    class_rgb_values=select_class_rgb_values,\n","    type=\"validation\",\n",")\n","\n","test_dataloader = DataLoader(test_dataset)\n","\n","# test dataset for visualization (without preprocessing transformations)\n","test_dataset_vis = BuildingsDataset(\n","    x_test_dir, y_test_dir, \n","    augmentation=get_validation_augmentation(),\n","    class_rgb_values=select_class_rgb_values,\n","    type=\"validation\",\n",")\n","\n","# get a random test image/mask index\n","random_idx = random.randint(0, len(test_dataset_vis)-1)\n","image, mask = test_dataset_vis[random_idx]\n","\n","visualize(\n","    original_image = image,\n","    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n","    one_hot_encoded_mask = reverse_one_hot(mask)\n",")\n","\n","# Notice the images / masks are 1536*1536 because of 18px padding on all sides. \n","# This is to ensure the input image dimensions to UNet model are a multiple of 2 (to account for pooling & transpose conv. operations)."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Center crop padded image / mask to original image dims\n","def crop_image(image, target_image_dims=[1500,1500,3]):\n","   \n","    target_size = target_image_dims[0]\n","    image_size = len(image)\n","    padding = (image_size - target_size) // 2\n","\n","    return image[\n","        padding:image_size - padding,\n","        padding:image_size - padding,\n","        :,\n","    ]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_preds_folder = 'newesttttt_sample_predictions/'\n","if not os.path.exists(sample_preds_folder):\n","    os.makedirs(sample_preds_folder)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["for idx in range(len(test_dataset)):\n","\n","    random_idx = random.randint(0, len(test_dataset)-1)\n","    image, gt_mask = test_dataset[random_idx]\n","    image_vis = crop_image(test_dataset_vis[random_idx][0].astype('uint8'))\n","    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n","    # Predict test image\n","    pred_mask = best_model(x_tensor)\n","    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n","    # Convert pred_mask from `CHW` format to `HWC` format\n","    pred_mask = np.transpose(pred_mask,(1,2,0))\n","    # Get prediction channel corresponding to building\n","    pred_building_heatmap = pred_mask[:,:,select_classes.index('building')]\n","    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n","    # Convert gt_mask from `CHW` format to `HWC` format\n","    gt_mask = np.transpose(gt_mask,(1,2,0))\n","    gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values))\n","    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n","    \n","    visualize(\n","        original_image = image_vis,\n","        ground_truth_mask = gt_mask,\n","        predicted_mask = pred_mask,\n","        predicted_building_heatmap = pred_building_heatmap\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["### Model Evaluation on Test Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["test_epoch = smp.utils.train.ValidEpoch(\n","    model,\n","    loss=loss, \n","    metrics=metrics, \n","    device=DEVICE,\n","    verbose=True,\n",")\n","\n","valid_logs = test_epoch.run(test_dataloader)\n","print(\"Evaluation on Test Data: \")\n","print(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\n","print(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")"]},{"cell_type":"markdown","metadata":{},"source":["### Plot Dice Loss & IoU Metric for Train vs. Val"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_logs_df = pd.DataFrame(train_logs_list)\n","valid_logs_df = pd.DataFrame(valid_logs_list)\n","train_logs_df.T"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20,8))\n","plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\n","plt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\n","plt.xlabel('Epochs', fontsize=21)\n","plt.ylabel('IoU Score', fontsize=21)\n","plt.title('IoU Score Plot', fontsize=21)\n","plt.legend(loc='best', fontsize=16)\n","plt.grid()\n","plt.savefig('newest_iou_score_plot.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(20,8))\n","plt.plot(train_logs_df.index.tolist(), train_logs_df.fscore.tolist(), lw=3, label = 'Train')\n","plt.plot(valid_logs_df.index.tolist(), valid_logs_df.fscore.tolist(), lw=3, label = 'Valid')\n","plt.xlabel('Epochs', fontsize=21)\n","plt.ylabel('F1 Score', fontsize=21)\n","plt.title('F1 Score Plot', fontsize=21)\n","plt.legend(loc='best', fontsize=16)\n","plt.grid()\n","plt.savefig('newset_f1_score_plot.png')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20,8))\n","plt.plot(train_logs_df.index.tolist(), train_logs_df.accuracy.tolist(), lw=3, label = 'Train')\n","plt.plot(valid_logs_df.index.tolist(), valid_logs_df.accuracy.tolist(), lw=3, label = 'Valid')\n","plt.xlabel('Epochs', fontsize=21)\n","plt.ylabel('Pixel Accuracy', fontsize=21)\n","plt.title('Pixel Accruracy Plot', fontsize=21)\n","plt.legend(loc='best', fontsize=16)\n","plt.grid()\n","plt.savefig('newest_paccuracy_plot.png')\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
